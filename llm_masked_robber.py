'''
Disclaimer: This tool is intended solely for academic and informational purposes. The analysis and descriptions of prompt injection techniques and related adversarial testing methods are provided to understand potential vulnerabilities in large language models (LLMs) and to advance the field of cybersecurity. Under no circumstances should the techniques described be used to exploit, manipulate, or compromise LLMs or other artificial intelligence systems outside of controlled, authorized research environments. All testing was conducted ethically, and with the aim of responsibly disclosing potential issues to improve the resilience and security of AI systems. The author does not assume any responsibility for misuse of the information presented.
'''


import torch
from transformers import RobertaTokenizer, RobertaForMaskedLM
import torch.nn.functional as F

# Load the tokenizer and model from local cache
# Ensure to use "roberta-base" as the model identifier for Facebook AI's RoBERTa model
tokenizer = RobertaTokenizer.from_pretrained("roberta-base", cache_dir="roberta-base", local_files_only=True)
model = RobertaForMaskedLM.from_pretrained("roberta-base", cache_dir="roberta-base", local_files_only=True)

# Input collection for the masked sentence and the number of top predictions to retrieve
masked_sentence = input("Type in your sentence and use __ to mask keywords: ")  # Prompt user for input sentence
top_k_input = input("Enter the number of top predictions to retrieve for each masked token: ")  # Prompt user for top-k value
top_k_input = int(top_k_input)  # Convert the input to an integer

def mask_and_predict_top_k_with_probs(text, masked_token="__", top_k=top_k_input):
    """
    Mask specified tokens in the text and predict the top-k words with probabilities for each masked word.
    
    Args:
    - text (str): The input text with a placeholder for the masked token.
    - masked_token (str): The placeholder text to be masked and predicted (default "__").
    - top_k (int): Number of top predictions to return for each masked token.
    
    Returns:
    - predictions (dict): A dictionary with masked token positions as keys and lists of tuples (word, probability) as values.
    """
    # Replace the masked token placeholder (__) with RoBERTa's <mask> token
    text_with_mask = text.replace(masked_token, tokenizer.mask_token)

    # Tokenize the input text with the <mask> token
    input_ids = tokenizer.encode(text_with_mask, return_tensors="pt")

    # Predict the masked tokens using the model
    with torch.no_grad():
        outputs = model(input_ids)
        predictions = outputs.logits  # Logits output from the model

    # Find the positions of the masked tokens in the input
    masked_indexes = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]

    # Dictionary to store top-k predictions and probabilities for each masked position
    predictions_dict = {}
    for idx in masked_indexes:
        # Extract the logits for the current masked position
        logits = predictions[0, idx]

        # Apply softmax to calculate probabilities for all tokens
        probs = F.softmax(logits, dim=-1)

        # Retrieve the top-k token IDs and their corresponding probabilities
        top_k_probs, top_k_token_ids = probs.topk(top_k)
        top_k_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_k_token_ids.tolist()]
        top_k_probs = top_k_probs.tolist()

        # Store the token-probability pairs for the current masked position
        predictions_dict[f"Masked Position {idx.item()}"] = list(zip(top_k_tokens, top_k_probs))

    return predictions_dict

def generate_prompts_with_predictions(text, predictions_dict, masked_token="__"):
    """
    Generate a list of prompts by substituting each masked position with its top predicted words.
    
    Args:
    - text (str): The original text with masked tokens.
    - predictions_dict (dict): A dictionary with masked token positions and their top-k predictions.
    - masked_token (str): The placeholder text to be replaced (default "__").
    
    Returns:
    - prompt_list (list): A list of prompts generated by filling in the masked words.
    """
    # Start with the original input text
    prompt_list = [text]
    for position, predictions in predictions_dict.items():
        new_prompt_list = []
        # Replace the masked token in all prompts with each top prediction
        for prompt in prompt_list:
            for word, _ in predictions:
                new_prompt = prompt.replace(masked_token, word, 1)
                new_prompt_list.append(new_prompt)
        prompt_list = new_prompt_list  # Update prompt list with new variations

    return prompt_list

# Generate predictions for the masked tokens in the input sentence
predicted_words_with_probs = mask_and_predict_top_k_with_probs(masked_sentence, top_k=top_k_input)

# Print the predictions with probabilities for each masked position
print("Top {top_k} Predictions with Probabilities for Each Masked Token:")
for position, predictions in predicted_words_with_probs.items():
    print(f"{position}:")
    for word, prob in predictions:
        print(f"  {word}: {prob:.4f}")
        # Log predictions to a CSV file
        with open('adversarial_prompts_masked_tokens.csv', 'a', encoding='utf-8') as f:
            f.write(f"{masked_sentence},{position},{word},{prob:.4f}\n")
            f.close()

# Generate all variations of the prompts by filling in the masked words
prompt_variations = generate_prompts_with_predictions(masked_sentence, predicted_words_with_probs)

# Print and log all generated prompts
print("\nGenerated Prompts with All Predicted Words:")
for prompt in prompt_variations:
    print(prompt)
    # Save generated prompts to a CSV file
    with open('adversarial_prompts.csv', 'a', encoding='utf-8') as f:
        f.write(f"{prompt}\n")
        f.close()
